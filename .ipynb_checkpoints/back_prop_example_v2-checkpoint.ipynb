{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cs231n 강의노트 Backpropagation 실제로 해보기\n",
    "아래의 그림은 forward pass 일 경우를 나타냅니다. 하나하나 실제로 코딩을 통해서 구현해 보도록 하겠습니다.\n",
    "<img src=\"images/original.png\" style=\"width:600;height:300px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** W와 x를 세팅합니다 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([[0.1, 0.5],[-0.3, 0.8]])\n",
    "x = np.array([[0.2],[0.4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W : \n",
      " [[ 0.1  0.5]\n",
      " [-0.3  0.8]]\n",
      "x : \n",
      " [[0.2]\n",
      " [0.4]]\n"
     ]
    }
   ],
   "source": [
    "print(\"W : \\n\", W)\n",
    "print(\"x : \\n\" ,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "첫번째 연산인 $Wx=q$를 수행해 보도록 하겠습니다. 매트릭스와 벡터를 곱하는 연산으로 뉴럴네트워크에서 기본적인 연산입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/wq=x.png\" style=\"width:600;height:300px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q : \n",
      " [[0.22]\n",
      " [0.26]]\n"
     ]
    }
   ],
   "source": [
    "q = np.dot(W, x)\n",
    "print(\"q : \\n\", q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 L2 Norm 연산과 관련된 부분입니다. 아래의 그림과 같습니다.\n",
    "<img src=\"images/l2norm.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "제곱해서 더하면 되기 때문에 numpy sum 연산을 이용했습니다. 위 슬라이드와 동일한 L2값을 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 : \n",
      " 0.11600000000000005\n"
     ]
    }
   ],
   "source": [
    "L2 = np.sum(q**2)\n",
    "print(\"L2 : \\n\", L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "조금 쉽게 접근해 보기 위해서 지금까지 그린 계산그래프(computational graph)를 조금 이해하기 쉬운 형태로 변형해 보겠습니다. $q_1$과 $q_2$를 따로따로 표현해보기 위해서 매트릭스 $W$를 행 벡터 (row vector) $w_1$과 $w_2$로 표현하겠습니다. \n",
    "먼저 $q_1$을 계산해 봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/q1.png\" style=\"width:300px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 $q_2$를 계산해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/q2.png\" style=\"height:250px;width:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 $q_1$과 $q_2$를 제곱한 뒤 더해서 결과를 L2로 표현하겠습니다.\n",
    "<img src=\"images/l2.png\" style=\"width:500;height:200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 이제부터 backpropagation을 그림을 통해 이해해 보도록 하겠습니다 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 공부한것처럼 $\\frac{\\partial L2}{\\partial L2} = 1$ 이고, 그 값은 덧셈노드를 만나서 그대로 전파되게 됩니다. 이는 아래의 그림과 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/back_1.png\" style=\"width:500;height:200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$q^2$을 미분하면 $2q$가 됩니다. 따라서 아래 그림과 같이 각각 $\\textit{local gradient}$ 인$2q$와 $\\textit{up stream gradient}$인 1이 곱해져서 $2q\\cdot1$ 이 됩니다. 이를 그림으로 표현하면 아래와 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/back_2.png\" style=\"width:500;height:200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벡터와 매트릭스가 연산 가능한 계산 그래프를 그려볼 수 있을 것 같습니다. 아래의 그림처럼 어떤 벡터의 L2 norm을 계산하는 것을 한번에 그리고 그것에 대한 미분도 한번에 표현할 수 있겠네요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/back_3.png\" style=\"width:500;height:200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 이를 이용해서 위 그림의 $\\frac{\\partial L2}{\\partial q}$를 계산해 보죠. 이는 다음과 같습니다. $q$값은 forward propagation에서 미리 계산해 두었었죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dL2_q = 2*q # q값은 forward연산때 미리 계산해 두었죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL2/dq : \n",
      " [[0.44]\n",
      " [0.52]]\n"
     ]
    }
   ],
   "source": [
    "print(\"dL2/dq : \\n\", dL2_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다시 원래의 cs231n 슬라이드를 보면 이 값이 맞다는 것을 확인 할 수 있습니다. 아래그림의 **확인**이 $\\frac{\\partial L2}{\\partial q}$ 결과 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/back_cs231n.png\" style=\"width:600;height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 매트릭스-벡터 곱, 즉 $Wx$에대한 backpropagation을 계산할 차례 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 연산의 local gradinet를 계산하기 위해 먼저 $q_1$과 $q_2$에 대해서 forward 연산을 다시 생각해 봅시다. 먼저 $q_1$ 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/back_q1.png\" style=\"width:500;height:150px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial q_1}{\\partial w_{11}}$과 $\\frac{\\partial q_1}{\\partial w_{12}}$의 계산은 다음과 같습니다. 아래의 그림의 번호 부분을 설명하면 다음과 같습니다.\n",
    "1. $q_1=w_{11}x_1+w_{12}x_2$를 각각 $w_{11}$과 $w_{12}$으로 미분하면\n",
    "2. $x_1$과 $x_2$가 된다\n",
    "3. 이는 $[x_1, x_2]$가 되고 이는 $x^T$로 표현 할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/back_q1_2.png\" style=\"width:400;height:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한번에 쓰면 결국,\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial q_1}{\\partial w_{1}}=x^T\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 위 연산에 $q_2$를 포함시키겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/back_q2.png\" style=\"width:200;height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결국 $q_1$과 $q_2$ 각각에 대한 $w_1$과 $w_2$의 미분이 $x^T$로 같게 되네요. $x^T$ 공통으로 쓰였으니 그렇게 되겠군요.\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial q_1}{\\partial w_{1}}=\\frac{\\partial q_2}{\\partial w_{2}}=x^T\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 계산그래프로 다시 $W$의 미분을 표현해 봅시다. 이는 아래의 그림처럼 표현해 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/back_red_highlight.png\" style=\"width:300;height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "붉은 형광색 부분을 local gradient x upstream gradient로 표현할 수 있고 $w_1$과 $w_2$로의 미분 모두 $x^T$이기 때문에"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/back_w1.png\" style=\"width:200;height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/back_q22.png\" style=\"width:200;height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두개를 묶어서 표현하면 다음과 같다.\n",
    "<img src=\"images/back_WW.png\" style=\"width:200;height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "계산 그래프로 표현하면 매트릭스-벡터 곱의 경우, 앞에 곱해진 매트릭스 방향의 그레디언트(즉, 이 예제에서 $W$ 방향으로)는 upstream gradient x 반대쪽 방향 입력의 전치(transpose)를 곱하면 된다. 뉴럴네트워크 구현 시 가장 많은 경우가 매트릭스-벡터 곱이기 때문에 알아두면 유용하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/back_graphw.png\" style=\"width:300;height:200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$\\frac{\\partial L2}{\\partial q}\\frac{\\partial q}{\\partial W}$** 는 위 그림을 이용하면 다음 수식으로 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dq_w = np.dot(dL2_q, x.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dq/dw : \n",
      " [[0.088 0.176]\n",
      " [0.104 0.208]]\n"
     ]
    }
   ],
   "source": [
    "print(\"dq/dw : \\n\", dq_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 마지막으로 $x$ 대한 미분을 계산해 보자 **\n",
    "$q_1$에 대한 local gradient를 계산해 보면 $w_1^T$가 된다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/back_x1_x2.png\" style=\"width:300;height:200px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 2번과 같이 $x$로의 미분은 자연스럽게 $w_1$이 되며 $x$와 동일한 모양이 되기 위해서 전치를 이용하였다.($w_1^T$) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/back_transpose_w1.png\" style=\"width:400;height:200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마찬가지 방식으로 $q_2$를 $x$로 미분한다면 $W_2^T$를 얻을 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/back_q2_w2.png\" style=\"width:300;height:100px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x$로의 미분은 결과가 두개이며 이를 각각 예제에 대하여 정리하면 다음과 같은 결과를 얻을 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/back_x_all.png\" style=\"width:300;height:150px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두개가 나온 이유는 단순하다. 식이 2개이고 나중에 합쳐졌기 때문이다. 계산 그래프로 표현해 보면 다음과 같다. 붉은 형광색으로 표현한 방향으로 그레디언트가 전달된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/back_merge.png\" style=\"width:300;height:200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 backpropagation 계산 시 만나게 되는 경우 합친다고 배웠다. 그 이유는 위 그림처럼 최종 1개의 loss로부터 전달되었기 때문이다. cs231n에서는 다음 슬라이드에서 그것을 설명하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cs231n_merge.png\" style=\"width:300;height:200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2개를 합치게 되면 다음과 같이 표현 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/backsum.png\" style=\"width:300;height:200px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 계산 그래프에 표현하면 다음과 같다(형광색 방향). $x$방향으로의 그레디언트가 계산되어야 추후 레이어를 쌓을때도 그레디언트가 전달 가능하다. 그러니 꼭 필요한 연산이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/back_final.png\" style=\"width:300;height:200px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**최종 그레디언 x**를 계산해 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_x = np.dot(W.T, dL2_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.112],\n",
       "       [ 0.636]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dq_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종 강의노트는 다음과 같다. 결과가 정확한지 확인해보라.\n",
    "<img src=\"images/cs231n_final.png\" style=\"width:400;height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 왜 Backpropagation을 구현해야 하는가\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
